{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from IPython import display\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "In this code, I used a part of the COCO dataset. This dataset contains a very large set of images, approximately 80K training images and 100 validation images, with multiple tags for each image.\n",
    "\n",
    "\n",
    "The code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  0.0039  0.0078  0.0039  ...   0.0471  0.0471  0.0314\n",
       "  0.0039  0.0039  0.0039  ...   0.0353  0.0353  0.0392\n",
       "  0.0039  0.0039  0.0039  ...   0.0392  0.0392  0.0510\n",
       "           ...             ⋱             ...          \n",
       "  0.7137  0.7294  0.7137  ...   0.1686  0.1843  0.1686\n",
       "  0.7059  0.6902  0.6863  ...   0.1765  0.1804  0.2039\n",
       "  0.6784  0.6667  0.6706  ...   0.1922  0.2157  0.2275\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0.1490  0.1490  0.1412  ...   0.0039  0.0039  0.0039\n",
       "  0.1451  0.1412  0.1373  ...   0.0039  0.0039  0.0039\n",
       "  0.1412  0.1373  0.1373  ...   0.0039  0.0039  0.0039\n",
       "           ...             ⋱             ...          \n",
       "  0.4392  0.4667  0.4549  ...   0.2588  0.2745  0.2863\n",
       "  0.4353  0.4235  0.4196  ...   0.2745  0.2980  0.3137\n",
       "  0.4118  0.4000  0.4000  ...   0.3020  0.3176  0.3020\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0.5294  0.5294  0.5294  ...   0.1451  0.1412  0.1333\n",
       "  0.5255  0.5333  0.5373  ...   0.1725  0.1451  0.1412\n",
       "  0.5373  0.5490  0.5451  ...   0.2314  0.1843  0.1608\n",
       "           ...             ⋱             ...          \n",
       "  0.0118  0.0078  0.0078  ...   0.5216  0.5294  0.5137\n",
       "  0.0078  0.0078  0.0118  ...   0.5098  0.5216  0.5216\n",
       "  0.0078  0.0118  0.0039  ...   0.5294  0.5255  0.4784\n",
       "[torch.FloatTensor of size 1x3x224x224]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Scale(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var\n",
    "#     return image_var.cuda()\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load annotations file for the training images.\n",
    "mscoco_train = json.load(open('data/annotations/train_captions.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "\n",
    "# Extract out the captions for the training images\n",
    "train_id_set = set(train_ids)\n",
    "train_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    if entry['image_id'] in train_id_set:\n",
    "        train_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the validation images.\n",
    "mscoco_val = json.load(open('data/annotations/val_captions.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# Extract out the captions for the validation images\n",
    "val_id_set = set(val_ids)\n",
    "val_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    if entry['image_id'] in val_id_set:\n",
    "        val_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the testing images\n",
    "mscoco_test = json.load(open('data/annotations/test_captions.json'))\n",
    "test_ids = [entry['id'] for entry in mscoco_test['images']]\n",
    "test_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_test['images']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We do the preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [sentence for caption_set in train_id_to_captions.values() for sentence in caption_set]\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentences, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a list of reference sentences, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = [word_tokenize(ref_sent.lower()) for ref_sent in reference_sentences]\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu(reference_tokenized, predicted_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Image Encoder\n",
    "\n",
    "Here I will work with the VGG-16 image classification CNN network first introduced in [Very Deep Convolutional Neural Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf) by K. Simonyan and A. Zisserman.\n",
    "\n",
    "Fairly straightforwardly, I load the pre-trained VGG model and indicate to PyTorch that we are using the model for inference rather than training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU (inplace)\n",
       "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU (inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU (inplace)\n",
       "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU (inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU (inplace)\n",
       "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU (inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU (inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU (inplace)\n",
       "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU (inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU (inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU (inplace)\n",
       "    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Linear (25088 -> 4096)\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Dropout (p = 0.5)\n",
       "    (3): Linear (4096 -> 4096)\n",
       "    (4): ReLU (inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "vgg_model = models.vgg16(pretrained=True).cuda()\n",
    "modified_classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])\n",
    "modified_classifier.eval()\n",
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup a Language Decoder\n",
    "\n",
    "We're going to buold a language decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "use_cuda = True\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, memory):\n",
    "        output = F.relu(input)\n",
    "        output, (hidden, memory) = self.lstm(output, (hidden, memory))\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, memory\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "decoder = DecoderLSTM(input_size=wordEncodingSize, hidden_size=300, output_size=len(vocabulary))\n",
    "decoder = decoder.cuda() if use_cuda else decoder\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = nn.Linear(4096, 300)\n",
    "training_vectors = []\n",
    "for i,image_id in enumerate(train_ids[:7000]):\n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(train_id_to_file[image_id])\n",
    "\n",
    "    # Run through the convolutional layers and resize the output.\n",
    "    features_output = vgg_model.features(img)\n",
    "    classifier_input = features_output.view(1, -1)\n",
    "\n",
    "    # Run through all but final classifier layers.\n",
    "    output = fc1(modified_classifier(classifier_input))\n",
    "    training_vectors.append(np.array(list(output.data.squeeze())))\n",
    "    if(i % 100 == 0):\n",
    "        print(i)\n",
    "# For simplicity, we convert this to a numpy array and save the result to a file.\n",
    "training_vectors = np.stack(training_vectors, axis=0)\n",
    "np.save(open('outputs/training_vectors', 'wb+'), training_vectors)\n",
    "# training_vectors = np.load('outputs/training_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we vectorize all of the validation images and write the results to a file.\n",
    "validation_vectors = []\n",
    "for image_id in (val_ids):\n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(val_id_to_file[image_id])\n",
    "\n",
    "    # Run through the convolutional layers and resize the output.\n",
    "    features_output = vgg_model.features(img)\n",
    "    classifier_input = features_output.view(1, -1)\n",
    "\n",
    "    # Run through all but final classifier layers.\n",
    "    output = fc1(modified_classifier(classifier_input))\n",
    "    validation_vectors.append(list(output.data.squeeze()))\n",
    "\n",
    "# For simplicity, we convert this to a numpy array and save the result to a file.\n",
    "validation_vectors = np.array(validation_vectors)\n",
    "np.save(open('outputs/validation_vectors', 'wb+'), validation_vectors)\n",
    "\n",
    "# validation_vectors = np.load('outputs/validation_vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train encoder-decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(decoder_input, \n",
    "          target_variable,\n",
    "          decoder_hidden, \n",
    "          decoder, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=w2v_embeddings, \n",
    "          teacher_force=True):\n",
    "\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    memory = Variable(torch.zeros(1, 1, 300))\n",
    "    memory = memory.cuda() if use_cuda else memory\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, memory = decoder(decoder_input, decoder_hidden, memory)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        \n",
    "        if teacher_force:\n",
    "            ni = target_variable[di].data[0]\n",
    "        else:          \n",
    "            ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 10\n",
    "for _ in range(num_epochs):\n",
    "    for i,image_id in enumerate(train_ids[:7000]):\n",
    "        for caption in train_id_to_captions[image_id][:5]:\n",
    "            numberized = preprocess_numberize(caption)\n",
    "            init_word = caption.split()[0].lower()\n",
    "            input_variable = Variable(torch.FloatTensor([[w2v_embeddings[word2index.get(init_word, 0)]]]))\n",
    "            input_variable = input_variable.cuda() if use_cuda else input_variable\n",
    "            \n",
    "            target_variable = Variable(torch.LongTensor(numberized[1:]))\n",
    "            target_variable = target_variable.cuda() if use_cuda else target_variable\n",
    "\n",
    "            hidden_variable = Variable(torch.FloatTensor([training_vectors[i]])).unsqueeze(0)\n",
    "            hidden_variable = hidden_variable.cuda() if use_cuda else hidden_variable\n",
    "\n",
    "            loss = train(input_variable,\n",
    "                         target_variable,\n",
    "                         hidden_variable,\n",
    "                         decoder, \n",
    "                         decoder_optimizer, \n",
    "                         criterion)\n",
    "        if i % 100 == 0:\n",
    "            print(i,loss)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MAP and Sampling Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(decoder, index, init_word, embeddings=w2v_embeddings, max_length=maxSequenceLength):\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index.get(init_word, 0)]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = Variable(torch.FloatTensor([validation_vectors[index]])).unsqueeze(0)\n",
    "    decoder_hidden = decoder_hidden.cuda() if use_cuda else decoder_hidden\n",
    "    memory = Variable(torch.zeros(1, 1, 300))\n",
    "    memory = memory.cuda() if use_cuda else memory\n",
    "    decoder_outputs = [word2index[init_word]]\n",
    "    decoder_outputs = decoder_outputs.cuda() if use_cuda else decoder_outputs\n",
    "    softmax = nn.Softmax()\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, memory = decoder(decoder_input, decoder_hidden, memory)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        decoder_outputs.append(ni)\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "            print(topi[0][0])\n",
    "\n",
    "    return \" \".join([vocabulary[word] for word in decoder_outputs])\n",
    "\n",
    "for i,image_id in enumerate(val_ids[:10]):\n",
    "    display.display(display.Image(val_id_to_file[image_id]))\n",
    "    for caption in val_id_to_captions[image_id][:5]:\n",
    "        print(inference(decoder, i, init_word=\"<SOS>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def sampling_inference(decoder, index, init_word, embeddings=w2v_embeddings, max_length=maxSequenceLength):\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index.get(init_word, 0)]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = Variable(torch.FloatTensor([validation_vectors[index]])).unsqueeze(0)\n",
    "    decoder_hidden = decoder_hidden.cuda() if use_cuda else decoder_hidden\n",
    "    \n",
    "    memory = Variable(torch.zeros(1, 1, 300))\n",
    "    memory = memory.cuda() if use_cuda else memory\n",
    "    \n",
    "    decoder_outputs = [word2index.get(init_word, 0)]\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, memory = decoder(decoder_input, decoder_hidden, memory)\n",
    "        probs = np.exp(decoder_output.data[0].cpu().numpy())\n",
    "        sample_sum = probs[0]\n",
    "        random_sample = random()\n",
    "        ni = 0\n",
    "        while sample_sum < random_sample:\n",
    "            ni += 1\n",
    "            sample_sum += probs[ni]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        decoder_outputs.append(ni)\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join([vocabulary[word] for word in decoder_outputs])\n",
    "\n",
    "for i,image_id in enumerate(val_ids[:10]):\n",
    "    display.display(display.Image(val_id_to_file[image_id]))\n",
    "    for caption in val_id_to_captions[image_id][:5]:\n",
    "        print(sampling_inference(decoder, i, init_word=\"<SOS>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate performance\n",
    "\n",
    "For validation images compute the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "for i,image_id in enumerate(val_ids):\n",
    "    for caption in val_id_to_captions[image_id][:5]:\n",
    "        generated_caption = inference(decoder, i, init_word=\"<SOS>\")\n",
    "        generated_caption = generated_caption.replace(' <EOS>','')\n",
    "        generated_caption = generated_caption.replace('<SOS> ','')\n",
    "        score += compute_bleu(caption, generated_caption)\n",
    "print(score/len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "for i,image_id in enumerate(val_ids):\n",
    "    for caption in val_id_to_captions[image_id]:\n",
    "        generated_caption = sampling_inference(decoder, i, init_word=\"<SOS>\")\n",
    "        generated_caption = generated_caption.replace(' <EOS>','')\n",
    "        generated_caption = generated_caption.replace('<SOS> ','')\n",
    "        score += compute_bleu(caption, generated_caption)\n",
    "print(score/len(val_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "    \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output.squeeze())\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vgg, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vgg = vgg\n",
    "        self.out = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self,image):\n",
    "        output = self.vgg(image)\n",
    "        out=self.out(output)\n",
    "        return out\n",
    "\n",
    "decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary))\n",
    "decoder = decoder.cuda() if use_cuda else decoder\n",
    "   \n",
    "encoder = Encoder(vgg_model, input_size=4096, hidden_size=300)\n",
    "encoder = encoder.cuda() if use_cuda else encoder\n",
    "\n",
    "for p in encoder.vgg.parameters():\n",
    "    p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from random import random\n",
    "def fast_image_loader(batch_ids): \n",
    "    images_array =np.zeros([len(batch_ids), 3, 224, 224])\n",
    "    for i,image_id in enumerate(batch_ids):\n",
    "        img = plt.imread(train_id_to_file.get(image_id))\n",
    "        img = resize(img, (224, 224))\n",
    "        images_array[i, :, :, :] = img.T\n",
    "#         print(i)\n",
    "    return images_array\n",
    "\n",
    "def train(input_variables, \n",
    "          target_variables,\n",
    "          indexed_list,\n",
    "          input_lens, \n",
    "          decoder,\n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings, \n",
    "          teacher_force=True):\n",
    "\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    images_array = fast_image_loader(indexed_list)\n",
    "    last_hidden = torch.stack([encoder(Variable(torch.FloatTensor(image).cuda()).view(1,3,224,224)) for image in images_array]).view(1,len(images_array),300)\n",
    "    \n",
    "#     # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]\n",
    "                                                for i in range(input_variables.size(1))]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = (last_hidden, last_hidden)\n",
    "    all_decoder_outputs = Variable(torch.zeros(*input_variables.size()))\n",
    "    if use_cuda:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        \n",
    "    all_decoder_outputs[0] = decoder_input\n",
    "        \n",
    "    # Iterate over the indices after the first.\n",
    "    for t in range(1,target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        if random() <= 0.3:\n",
    "            decoder_input = input_variables[t].unsqueeze(0)\n",
    "        else:\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "             decoder_input = torch.stack([Variable(torch.FloatTensor(embeddings[ni])).cuda()\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "        \n",
    "        # Save the decoder output\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        \n",
    "    loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        target_variable.transpose(0,1).contiguous(), \n",
    "                        Variable(torch.LongTensor(input_lens)).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "def pad_seq(arr, length, pad_token):\n",
    "    \"\"\"\n",
    "    Pad an array to a length with a token.\n",
    "    \"\"\"\n",
    "    if len(arr) == length:\n",
    "        return np.array(arr)\n",
    "    \n",
    "    return np.concatenate((arr, [pad_token]*(length - len(arr))))\n",
    "\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "for _ in range(num_epochs):\n",
    "    for i in range(len(train_ids[:7000])//batch_size):\n",
    "        # Get the sentences in the batch\n",
    "        indexes = train_ids[i*batch_size:(i+1)*batch_size]\n",
    " \n",
    "        sentences = []\n",
    "        for ind in indexes:\n",
    "            for j in range(5):\n",
    "                sentences += [(ind,  train_id_to_captions[ind][j])]\n",
    " \n",
    "        # Get the sentence lengths\n",
    "        sentence_lens = [(sentence[0], len(preprocess_numberize(sentence[1]))) for sentence in sentences]\n",
    "        \n",
    "        # Sort by the sentence lengths\n",
    "        sorted_indices = sorted(list(range(len(sentence_lens))), key=lambda i: sentence_lens[i][1], reverse=True)\n",
    "        sentences = [sentences[i] for i in sorted_indices if sentence_lens[i][1] > 0]\n",
    "        \n",
    "        # Filter out 0 sentence lengths\n",
    "        sentence_lens = [sentence_lens[i][1] for i in sorted_indices if sentence_lens[i][1] > 0]\n",
    "\n",
    "        # Determine length to pad everything to\n",
    "        max_len = max(sentence_lens)\n",
    "        \n",
    "        # Preprocess all of the sentences in each batch\n",
    "        one_hot_embedded_list = [(sentence[0], preprocess_one_hot(sentence[1])) for sentence in sentences]\n",
    "        one_hot_embedded_list_padded = [pad_seq(embed[1], max_len, np.zeros(len(vocabulary))) \n",
    "                                        for embed in one_hot_embedded_list]\n",
    "                \n",
    "        numberized_list = [(sentence[0], preprocess_numberize(sentence[1])) for sentence in sentences]\n",
    "        numberized_list_padded = [pad_seq(numb[1], max_len, 0).astype(torch.LongTensor) for numb in numberized_list]\n",
    "        \n",
    "        # Convert to variables\n",
    "        indexed_list = [sentence[0] for sentence in sentences]\n",
    "\n",
    "        input_variable = Variable(torch.FloatTensor(one_hot_embedded_list_padded))\n",
    "        input_variable = input_variable.cuda() if use_cuda else input_variable\n",
    "        target_variable = Variable(torch.LongTensor(numberized_list_padded))\n",
    "        target_variable = target_variable.cuda() if use_cuda else target_variable\n",
    "        # Transpose from batch_size x max_seq_len x vocab_size to max_seq_len x batch_size x vocab_size\n",
    "        input_variable = input_variable.transpose(0, 1)\n",
    "        target_variable = target_variable.transpose(0, 1)\n",
    "        \n",
    "        loss = train(input_variable,\n",
    "                     target_variable,\n",
    "                     indexed_list,\n",
    "                     sentence_lens,\n",
    "                     decoder,\n",
    "                     decoder_optimizer, \n",
    "                     criterion)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(i,loss)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
